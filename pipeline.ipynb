{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import seaborn as sns\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import sqlite3\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "db_path = 'data.db'\n",
    "connection = sqlite3.connect(db_path)\n",
    "\n",
    "queryplayers = \"SELECT * FROM players\"\n",
    "players = pd.read_sql_query(queryplayers, connection)\n",
    "\n",
    "queryTeams = \"SELECT * FROM teams\"\n",
    "teams = pd.read_sql_query(queryTeams, connection)\n",
    "\n",
    "queryplayers_teams = \"SELECT * FROM players_teams\"\n",
    "players_teams = pd.read_sql_query(queryplayers_teams, connection)\n",
    "\n",
    "queryawards = \"SELECT * FROM awards_players\"\n",
    "awards_players = pd.read_sql_query(queryawards, connection)\n",
    "\n",
    "querycoaches = \"SELECT * FROM coaches\"\n",
    "coaches = pd.read_sql_query(querycoaches, connection)\n",
    "\n",
    "queryseries_post = \"SELECT * FROM coaches\"\n",
    "series_post = pd.read_sql_query(queryseries_post, connection)\n",
    "\n",
    "queryteams_post = \"SELECT * FROM coaches\"\n",
    "teams_post = pd.read_sql_query(queryteams_post, connection)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Awards_players\n",
    "\n",
    "\n",
    "- corrected name of a prize\n",
    "- drop lgID\n",
    "- calculate the number of prizes that each player has (having in consideration the year)\n",
    "- drop award (name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "awards_players['award'] = awards_players['award'].replace(\"Kim Perrot Sportsmanship\", \"Kim Perrot Sportsmanship Award\")\n",
    "awards_players = awards_players.drop(columns=['lgID'], axis=1)\n",
    "\n",
    "award_weights = {\n",
    "    \"All-Star Game Most Valuable Player\": 2,\n",
    "    \"Coach of the Year\": 3,\n",
    "    \"Defensive Player of the Year\": 4,\n",
    "    \"Kim Perrot Sportsmanship Award\": 1,\n",
    "    \"Most Improved Player\": 2,\n",
    "    \"Most Valuable Player\": 5,\n",
    "    \"Rookie of the Year\": 3,\n",
    "    \"Sixth Woman of the Year\": 2,\n",
    "    \"WNBA Finals Most Valuable Player\": 4,\n",
    "    \"WNBA All-Decade Team\": 1,\n",
    "    \"WNBA All Decade Team Honorable Mention\": 1,\n",
    "}\n",
    "\n",
    "# Sort the DataFrame by playerID and year\n",
    "awards_players.sort_values(by=[\"playerID\", \"year\"], inplace=True)\n",
    "\n",
    "awards_players['award_weight'] = awards_players['award'].map(award_weights)\n",
    "awards_players['num_prizes'] = awards_players.groupby('playerID')['award_weight'].cumsum()\n",
    "awards_players = awards_players.drop_duplicates(subset=['playerID', 'year'], keep='last')\n",
    "\n",
    "awards_players = awards_players.drop(columns=['award', 'award_weight'], axis=1)\n",
    "\n",
    "awards_players.to_csv('./Modified/modified_awards.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Players\n",
    "\n",
    "- drop college, collegeOther, pos and lastseason\n",
    "- remove who has height = 0\n",
    "- replace height error (9) with the average\n",
    "- replace weight error (0) with the average\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "players['height'] = pd.to_numeric(players['height'], errors='coerce')\n",
    "players = players.drop(columns=['college', 'pos', 'collegeOther', 'lastseason', 'firstseason'], axis=1)\n",
    "\n",
    "players = players[players['height'] > 0]\n",
    "\n",
    "# Calculate the mean of non-zero heights\n",
    "mean_height = players[(players['height'] != 0) | (players['height'] != 9)]['height'].mean()\n",
    "# Replace 9 with the mean height\n",
    "players['height'] = players['height'].replace(9, mean_height)\n",
    "\n",
    "# Calculate the mean of non-zero weights\n",
    "mean_weight = players[players['weight'] != 0]['weight'].mean()\n",
    "players['weight'] = players['weight'].replace(0, mean_weight)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Players_teams\n",
    "\n",
    "- statistics: info about the previous year\n",
    "    - if never appeared before, null ( we doesnt know anything about that player at the beginning of the season)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "players_teams = players_teams.drop(columns=['lgID', 'PostGP','PostGS','PostMinutes','PostPoints','PostoRebounds','PostdRebounds','PostRebounds','PostAssists','PostSteals','PostBlocks','PostTurnovers','PostPF','PostfgAttempted','PostfgMade','PostftAttempted','PostftMade','PostthreeAttempted','PostthreeMade','PostDQ'], axis=1)\n",
    "players_teams.sort_values(by=[\"playerID\", \"year\", \"stint\"], inplace=True)\n",
    "testing_set_with_stats = pd.DataFrame()\n",
    "\n",
    "previous_columns = ['GP', 'GS', 'minutes', 'points', 'oRebounds', 'dRebounds', 'rebounds', 'assists', 'steals', 'blocks', 'turnovers', 'PF', 'fgAttempted', 'fgMade', 'ftAttempted', 'ftMade', 'threeAttempted', 'threeMade', 'dq']\n",
    "\n",
    "players = players.rename(columns={'bioId': 'playerID'})\n",
    "\n",
    "for year in range(2,12):\n",
    "\n",
    "    players_in_year = players_teams[players_teams['year'] == year]\n",
    "\n",
    "    players_in_year = players_teams[players_teams['playerID'].isin(players_in_year['playerID'])]\n",
    "\n",
    "    players_in_year = players_in_year[players_in_year['year'] < year]\n",
    "\n",
    "    last_season_per_player = players_in_year.groupby('playerID')['year'].max().reset_index()\n",
    "\n",
    "    last_season_stats = players_teams.merge(last_season_per_player, on=['playerID', 'year'], how='right')\n",
    "\n",
    "    last_year_sum = last_season_stats.groupby('playerID').sum()[previous_columns].reset_index()\n",
    "\n",
    "    testing_set_for_year = last_year_sum.merge(players, on=['playerID'], how='left')\n",
    "    \n",
    "    testing_set_for_year['year'] = year\n",
    "\n",
    "    testing_set_with_stats = pd.concat([testing_set_with_stats, testing_set_for_year], ignore_index=True)\n",
    "    \n",
    "\n",
    "players_teams= players_teams[['playerID', 'year', 'tmID']]\n",
    "\n",
    "testing_set_with_stats = testing_set_with_stats.merge(players_teams, on=['playerID', 'year'], how='right')\n",
    "\n",
    "players_teams = testing_set_with_stats\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- add a column called Age, considering the year of the season. Calculated through the merge between two datasets~\n",
    "- number of prizes of each player (according to the year)\n",
    "- drop birthdate and deathdate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_age(row):\n",
    "    birth_date = row['birthDate']\n",
    "    # Check if birth_date is not NaN (i.e., it's not a float)\n",
    "    if not pd.isnull(birth_date):\n",
    "        birth_year = int(str(birth_date)[:4])\n",
    "        age = 2000 + row['year'] - birth_year\n",
    "        return age\n",
    "    else:\n",
    "        # Handle NaN or float values in 'birthDate' column\n",
    "        return None \n",
    "\n",
    "\n",
    "players_teams[\"birthDate\"] = players_teams[\"playerID\"].map(players.set_index(\"playerID\")[\"birthDate\"])\n",
    "\n",
    "players_teams[\"height\"] = players_teams[\"playerID\"].map(players.set_index(\"playerID\")[\"height\"])\n",
    "\n",
    "players_teams[\"weight\"] = players_teams[\"playerID\"].map(players.set_index(\"playerID\")[\"weight\"])\n",
    "\n",
    "# Apply the function to each row\n",
    "players_teams['age'] = players_teams.apply(calculate_age, axis=1)\n",
    "\n",
    "\n",
    "merged_data = players_teams.merge(awards_players, on=['playerID', 'year'], how='left')\n",
    "\n",
    "\n",
    "merged_data['num_prizes'] = merged_data.groupby('playerID')['num_prizes'].ffill()\n",
    "\n",
    "\n",
    "merged_data['num_prizes'].fillna(0, inplace=True)\n",
    "\n",
    "\n",
    "merged_data['num_prizes'] = merged_data.groupby('playerID')['num_prizes'].shift(1)\n",
    "\n",
    "\n",
    "merged_data.fillna(0, inplace=True)\n",
    "\n",
    "\n",
    "merged_data = merged_data.rename(columns={'num_prizes': 'players_prizes'})\n",
    "\n",
    "merged_data = merged_data.drop('birthDate', axis=1)\n",
    "merged_data = merged_data.drop('deathDate', axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coaches\n",
    "\n",
    "- drop lgID, stint, post_wins and post_losses\n",
    "- create column num_prizes (merging awards_players)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coaches = coaches.drop(columns=['lgID'], axis=1)\n",
    "#coaches = coaches.drop(columns=['stint'], axis=1)\n",
    "\n",
    "coaches = coaches.merge(awards_players, left_on=['coachID', 'year'], right_on=['playerID', 'year'], how='left')\n",
    "coaches.sort_values(by=[\"coachID\", \"year\"], inplace=True)\n",
    "\n",
    "coaches['num_prizes'] = coaches.groupby('coachID')['num_prizes'].ffill()\n",
    "\n",
    "# Fill NaN values with 0 for players with no prizes \n",
    "coaches['num_prizes'].fillna(0, inplace=True)\n",
    "\n",
    "coaches['num_prizes'] = coaches.groupby('coachID')['num_prizes'].shift(1)\n",
    "\n",
    "coaches['num_prizes'].fillna(0, inplace=True)\n",
    "\n",
    "\n",
    "coaches = coaches.drop(columns=['playerID'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coaches.sort_values(by=[\"coachID\", \"year\", \"stint\"], inplace=True)\n",
    "\n",
    "testing_set_with_stats = pd.DataFrame()\n",
    "\n",
    "previous_columns = ['won', 'lost', 'post_wins', 'post_losses']\n",
    "\n",
    "for year in range(2,11):\n",
    "\n",
    "    players_in_year = coaches[coaches['year'] == year]\n",
    "\n",
    "    players_in_year = coaches[coaches['coachID'].isin(players_in_year['coachID'])]\n",
    "\n",
    "    players_in_year = players_in_year[players_in_year['year'] < year]\n",
    "\n",
    "    last_season_per_player = players_in_year.groupby('coachID')['year'].max().reset_index()\n",
    "\n",
    "    last_season_stats = coaches.merge(last_season_per_player, on=['coachID', 'year'], how='right')\n",
    "    last_season_stats['won'] = pd.to_numeric(last_season_stats['won'], errors='coerce')\n",
    "    last_season_stats['lost'] = pd.to_numeric(last_season_stats['lost'], errors='coerce')\n",
    "    last_season_stats['post_wins'] = pd.to_numeric(last_season_stats['post_wins'], errors='coerce')\n",
    "    last_season_stats['post_losses'] = pd.to_numeric(last_season_stats['post_losses'], errors='coerce')\n",
    "\n",
    "    last_year_sum = last_season_stats.groupby('coachID').sum()[previous_columns].reset_index()\n",
    "\n",
    "    #testing_set_for_year = last_year_sum.merge(players, on='playerID', how='left')\n",
    "    \n",
    "    last_year_sum['year'] = year\n",
    "\n",
    "    testing_set_with_stats = pd.concat([testing_set_with_stats, last_year_sum], ignore_index=True)\n",
    "    \n",
    "\n",
    "testing_set_with_stats = testing_set_with_stats.merge(coaches, on=['coachID', 'year'], how='right')\n",
    "\n",
    "testing_set_with_stats = testing_set_with_stats.drop(columns=['won_y', 'lost_y', 'post_wins_y', 'post_losses_y'], axis=1)\n",
    "\n",
    "testing_set_with_stats = testing_set_with_stats.dropna(thresh=6, axis=0)\n",
    "\n",
    "coaches = testing_set_with_stats\n",
    "\n",
    "coaches = coaches.rename(columns={'won_x': 'coach_won'})\n",
    "coaches = coaches.rename(columns={'lost_x': 'coach_lost'})\n",
    "coaches = coaches.rename(columns={'post_wins_x': 'coach_post_wins'})\n",
    "coaches = coaches.rename(columns={'post_losses_x': 'coach_post_losses'})\n",
    "\n",
    "\n",
    "coaches.to_csv('./Modified/modified_coaches.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teams\n",
    "\n",
    "- drop seeded, confID, divID, franchID, arena, lgID and name\n",
    "- corrected error: one season it was registered two winners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teams = teams.drop(columns=['seeded', 'divID', 'franchID', 'arena', 'lgID', 'name', 'tmORB'], axis=1)\n",
    "teams = teams.drop(columns=['tmDRB'], axis=1)\n",
    "teams = teams.drop(columns=['tmTRB'], axis=1)\n",
    "teams = teams.drop(columns=['opptmORB'], axis=1)\n",
    "teams = teams.drop(columns=['opptmDRB'], axis=1)\n",
    "teams = teams.drop(columns=['opptmTRB'], axis=1)\n",
    "\n",
    "\n",
    "stats_teams = ['o_fgm','o_fga','o_ftm','o_fta','o_3pm','o_3pa','o_oreb','o_dreb','o_reb','o_asts','o_pf','o_stl','o_to','o_blk','o_pts','d_fgm','d_fga','d_ftm','d_fta','d_3pm','d_3pa','d_oreb','d_dreb','d_reb','d_asts','d_pf','d_stl','d_to','d_blk','d_pts','GP','homeW','homeL','awayW','awayL','confW','confL','min']\n",
    "teams = teams.drop(columns=stats_teams, axis=1)\n",
    "\n",
    "columns_to_shift = ['won', 'lost', 'attend', 'rank']\n",
    "columns_post = ['firstRound', 'semis', 'finals']\n",
    "\n",
    "teams.sort_values(by=[\"tmID\", \"year\"])\n",
    "\n",
    "\n",
    "\"\"\" for col in columns_to_shift:\n",
    "    teams[f'prev_{col}'] = teams.groupby('tmID')[col].shift(1)\n",
    "    teams[f'prev_{col}'].fillna(0, inplace=True) \"\"\"\n",
    "\n",
    "\n",
    "for col in columns_post:\n",
    "    teams[f'prev_{col}'] = label_encoder.fit_transform(teams.groupby('tmID')[col].shift(1))\n",
    "\n",
    "teams = teams.drop(columns=columns_post, axis=1)\n",
    "\n",
    "teams = teams.drop(columns=columns_to_shift, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- calculate the number of prizes a team has (total of prizes the players have), creating the column num_prizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teams['year'] = pd.to_numeric(teams['year'], errors='coerce')\n",
    "teams.sort_values(by=[\"tmID\", \"year\"])\n",
    "\n",
    "playerAndTeams = merged_data.merge(teams, on=[\"tmID\", \"year\"])\n",
    "\n",
    "# Group by team and year, summing the number of prizes\n",
    "team_prizes = playerAndTeams.groupby(['tmID', 'year'])['players_prizes'].sum().reset_index()\n",
    "\n",
    "# Sort the result by year\n",
    "team_prizes.sort_values(by='year', inplace=True)\n",
    "\n",
    "teams = teams.merge(team_prizes, on=[\"tmID\", \"year\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- calculate the sum of the statistics of the players of each team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the columns to sum\n",
    "#sum_columns = ['previous_GP', 'previous_GS', 'previous_minutes', 'previous_points', 'previous_oRebounds', 'previous_dRebounds', 'previous_rebounds', 'previous_assists', 'previous_steals', 'previous_blocks', 'previous_turnovers', 'previous_PF', 'previous_fgAttempted', 'previous_fgMade', 'previous_ftAttempted', 'previous_ftMade', 'previous_threeAttempted', 'previous_threeMade', 'previous_dq']\n",
    "\n",
    "sum_columns = ['GP', 'GS', 'minutes', 'points', 'oRebounds', 'dRebounds', 'rebounds', 'assists', 'steals', 'blocks', 'turnovers', 'PF', 'fgAttempted', 'fgMade', 'ftAttempted', 'ftMade', 'threeAttempted', 'threeMade', 'dq']\n",
    "\n",
    "# Sum the columns for each team and year\n",
    "summed_columns = playerAndTeams.groupby(['tmID', 'year'])[sum_columns].sum().reset_index()\n",
    "\n",
    "summed_columns.columns = ['tmID', 'year'] + ['sum_' + col for col in sum_columns]\n",
    "\n",
    "# Merge the summed columns back into the 'teams' table\n",
    "teams = pd.merge(teams, summed_columns, how='left', on=['tmID', 'year'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- calculate the mean of the height, weight and age of the players of each team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "team_means = playerAndTeams.groupby(['tmID', 'year']).agg({\n",
    "    'height': 'mean',\n",
    "    'weight': 'mean',\n",
    "    'age': 'mean',\n",
    "}).reset_index()\n",
    "teams = pd.merge(teams, team_means, on=['tmID', 'year'], how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- add to the table teams the number of prizes the coach has\n",
    "- number of victories and defeats of coach in previous year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" teams = pd.merge(teams, coaches[['tmID', 'year', 'coach_won', 'coach_lost']], on=['tmID', 'year'], how='left')\n",
    "\n",
    "teams['coach_won'] = teams['coach_won'].fillna(0)\n",
    "teams['coach_lost'] = teams['coach_lost'].fillna(0) \"\"\"\n",
    "\n",
    "merged_df = pd.merge(teams, coaches, on=['year', 'tmID'], how='left')\n",
    "\n",
    "merged_df['num_prizes'] = merged_df['num_prizes'].fillna(0)\n",
    "\n",
    "teams['coaches_prizes'] = merged_df['num_prizes']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teams.to_csv('./Modified/modified_teams.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- enconde categorical columns\n",
    "- split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Encode the 'playoff' column\n",
    "label_encoder_playoff = LabelEncoder()\n",
    "label_encoder_tmID = LabelEncoder()\n",
    "label_encoder_confID = LabelEncoder()\n",
    "\n",
    "teams['playoff'] = label_encoder_playoff.fit_transform(teams['playoff'])\n",
    "teams['tmID'] = label_encoder_tmID.fit_transform(teams['tmID'])\n",
    "teams['confID'] = label_encoder_confID.fit_transform(teams['confID'])\n",
    "\n",
    "\n",
    "columns_to_normalize = [\n",
    "    'sum_GP', 'sum_GS', 'sum_minutes', 'sum_points', 'sum_oRebounds',\n",
    "    'sum_dRebounds', 'sum_rebounds', 'sum_assists', 'sum_steals', 'sum_blocks',\n",
    "    'sum_turnovers', 'sum_PF', 'sum_fgAttempted', 'sum_fgMade', 'sum_ftAttempted',\n",
    "    'sum_ftMade', 'sum_threeAttempted', 'sum_threeMade', 'sum_dq'\n",
    "]\n",
    "\n",
    "teams[columns_to_normalize] = (teams[columns_to_normalize] - teams[columns_to_normalize].mean()) / teams[columns_to_normalize].std()\n",
    "\n",
    "\n",
    "train_data = teams[teams['year'].between(2, 9)]\n",
    "#train_data = train_data[train_data['year'] > 1]\n",
    "test_data = teams[teams['year'] == 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, auc\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Use the same train_data and test_data from your code\n",
    "X_train = train_data.drop(['playoff'], axis=1)\n",
    "y_train = train_data['playoff']\n",
    "X_test = test_data.drop(['playoff'], axis=1)\n",
    "y_test = test_data['playoff']\n",
    "\n",
    "# Define a list of classifiers\n",
    "classifiers = [\n",
    "    (\"Logistic Regression\", LogisticRegression(solver='liblinear', random_state=0), {'C': [0.001, 0.01, 0.1, 1, 10]}),\n",
    "    (\"Support Vector Machine\", SVC(kernel='linear', random_state=0), {'C': [0.001, 0.01, 0.1, 1, 10]}),\n",
    "    (\"Decision Tree\", DecisionTreeClassifier(random_state=0), {'max_depth': [None, 5, 10, 15]}),\n",
    "    (\"Random Forest\", RandomForestClassifier(random_state=0), {'n_estimators': [50, 100, 200], 'max_depth': [None, 5, 10]}),\n",
    "    (\"K-Nearest Neighbors\", KNeighborsClassifier(), {'n_neighbors': [3, 5, 7]}),\n",
    "    (\"Naive Bayes\", GaussianNB(), {})\n",
    "]\n",
    "\n",
    "# Create an empty dictionary to store performance metrics\n",
    "performance_metrics = {}\n",
    "roc_curve_data = []\n",
    "\n",
    "# Initialize variables to calculate averages\n",
    "average_accuracy = 0\n",
    "average_roc_auc = 0\n",
    "\n",
    "for name, clf, param_grid in classifiers:\n",
    "    start_time = time.time()\n",
    "    # Perform Grid Search\n",
    "    grid_search = GridSearchCV(clf, param_grid, cv=5, scoring='roc_auc')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Get the best model\n",
    "    best_clf = grid_search.best_estimator_\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "\n",
    "    # Use the best model to make predictions\n",
    "    predictions = best_clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    average_accuracy += accuracy\n",
    "\n",
    "    # Calculate ROC curve and AUC\n",
    "    if hasattr(best_clf, \"predict_proba\"):\n",
    "        y_score = best_clf.predict_proba(X_test)[:, 1]\n",
    "    else:\n",
    "        y_score = best_clf.decision_function(X_test)\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_score)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    average_roc_auc += roc_auc\n",
    "\n",
    "    # Store performance metrics in a dictionary\n",
    "    performance_metrics[name] = {\n",
    "        \"Best Parameters\": grid_search.best_params_,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"ROC AUC\": roc_auc,\n",
    "        \"Confusion Matrix\": confusion_matrix(y_test, predictions),\n",
    "        \"Classification Report\": classification_report(y_test, predictions),\n",
    "        \"ROC Curve\": (fpr, tpr),\n",
    "        \"Elapsed Time\": elapsed_time,\n",
    "        \"Model\": best_clf\n",
    "    }\n",
    "    # Append ROC curve data for later averaging\n",
    "    roc_curve_data.append((fpr, tpr))\n",
    "\n",
    "# Calculate averages\n",
    "average_accuracy /= len(classifiers)\n",
    "average_roc_auc /= len(classifiers)\n",
    "\n",
    "# Print the performance metrics and create ROC curve graphs\n",
    "for name, metrics in performance_metrics.items():\n",
    "    print(f\"{name} Performance Metrics:\")\n",
    "    print(f\"Best Parameters: {metrics['Best Parameters']}\")\n",
    "    print(f\"Accuracy: {metrics['Accuracy']}\")\n",
    "    print(f\"ROC AUC: {metrics['ROC AUC']}\")\n",
    "    print(f\"Elapsed Time: {metrics['Elapsed Time']:.2f} seconds\") \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(metrics['Confusion Matrix'], annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f'{name} Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.show()\n",
    "    print(\"Classification Report:\")\n",
    "    print(metrics['Classification Report'])\n",
    "\n",
    "    # Plot ROC Curve\n",
    "    plt.figure()\n",
    "    fpr, tpr = metrics[\"ROC Curve\"]\n",
    "    plt.plot(fpr, tpr, lw=2, label=f'{name} (AUC = {metrics[\"ROC AUC\"]:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'{name} ROC Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    \n",
    "plt.figure()\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "\n",
    "# Calculate and plot the average ROC curve\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "mean_tpr = np.mean([np.interp(mean_fpr, fpr, tpr) for fpr, tpr in roc_curve_data], axis=0)\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "plt.plot(mean_fpr, mean_tpr, color='b', label=f'Average ROC Curve (AUC = {mean_auc:.2f})')\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Average ROC Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Print averages\n",
    "print(f\"Average Accuracy: {average_accuracy:.2f}\")\n",
    "print(f\"Average ROC AUC: {average_roc_auc:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Year 11 (new year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through all models\n",
    "for name, metrics in performance_metrics.items():\n",
    "    current_model = metrics[\"Model\"]\n",
    "\n",
    "    train_data = teams[teams['year'] != 11]\n",
    "    test_data = teams[teams['year'] == 11]\n",
    "\n",
    "    X_train = train_data.drop(['playoff'], axis=1)\n",
    "    y_train = train_data['playoff']\n",
    "    X_test = test_data.drop(['playoff'], axis=1)\n",
    "\n",
    "    # Train the current model\n",
    "    current_model.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions using the current model\n",
    "    predictions = current_model.predict(X_test)\n",
    "\n",
    "    # Create a submission DataFrame for the current model\n",
    "    submission_df = pd.DataFrame({\n",
    "        'tmID': test_data['tmID'],\n",
    "        'playoff': predictions\n",
    "    })\n",
    "\n",
    "    # Decode team IDs if necessary\n",
    "    decoded_teams = label_encoder_tmID.inverse_transform(submission_df['tmID'])\n",
    "    submission_df['tmID'] = decoded_teams\n",
    "\n",
    "    # Save the submission file for the current model\n",
    "    submission_df.to_csv(f'submission_{name}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(solver='liblinear', random_state=0)\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Step 6: Make predictions\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "inputs = teams.columns\n",
    "feature_importance = abs(logreg.coef_[0])\n",
    "feature_importance = sorted(zip(inputs, feature_importance), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"Feature importance:\")\n",
    "for i in range(len(feature_importance)):\n",
    "    print(f\"{feature_importance[i][0]}: {feature_importance[i][1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data.to_csv('./Modified/players_playersTeams.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a correlation matrix for all columns\n",
    "correlation_matrix = train_data.corr()\n",
    "\n",
    "# Set the size of the heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Create a heatmap using seaborn\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.3)\n",
    "\n",
    "# Set the title\n",
    "plt.title(\"Correlation Matrix\")\n",
    "\n",
    "# Show the heatmap\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" teams_before = pd.read_csv('./Dataset/teams.csv')\n",
    "\n",
    "print(teams_before.columns)\n",
    "teams_before['playoff'] = label_encoder.fit_transform(teams_before['playoff'])\n",
    "#teams['previous_playoff'] = label_encoder.fit_transform(teams['previous_playoff'])\n",
    "teams_before['tmID'] = label_encoder.fit_transform(teams_before['tmID'])\n",
    "teams_before['confID'] = label_encoder.fit_transform(teams_before['confID'])\n",
    "teams_before['lgID'] = label_encoder.fit_transform(teams_before['lgID'])\n",
    "teams_before['franchID'] = label_encoder.fit_transform(teams_before['franchID'])\n",
    "teams_before['firstRound'] = label_encoder.fit_transform(teams_before['firstRound'])\n",
    "teams_before['semis'] = label_encoder.fit_transform(teams_before['semis'])\n",
    "teams_before['finals'] = label_encoder.fit_transform(teams_before['finals'])\n",
    "teams_before['name'] = label_encoder.fit_transform(teams_before['name'])\n",
    "teams_before['arena'] = label_encoder.fit_transform(teams_before['arena'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_data = teams_before[teams_before['year'] <= 9]\n",
    "test_data = teams_before[teams_before['year'] == 10]\n",
    "\n",
    "# Use the same train_data and test_data from your code\n",
    "X_train = train_data.drop(['playoff'], axis=1)\n",
    "y_train = train_data['playoff']\n",
    "X_test = test_data.drop(['playoff'], axis=1)\n",
    "y_test = test_data['playoff']\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Initialize the imputer with a strategy (e.g., mean, median, most_frequent, constant)\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Fit and transform the imputer on your training data\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "\n",
    "# Transform the imputer on your test data (using statistics learned from training data)\n",
    "X_test_imputed = imputer.transform(X_test)\n",
    "\n",
    "# Now, you can use X_train_imputed and X_test_imputed for training and testing your models\n",
    "\n",
    "# Define a list of classifiers\n",
    "classifiers = [\n",
    "    (\"Logistic Regression\", LogisticRegression(solver='liblinear', random_state=0)),\n",
    "    (\"Decision Tree\", DecisionTreeClassifier(random_state=0)),\n",
    "    (\"Random Forest\", RandomForestClassifier(random_state=0)),\n",
    "    (\"Support Vector Machine\", SVC(kernel='linear', random_state=0)),\n",
    "    (\"K-Nearest Neighbors\", KNeighborsClassifier())\n",
    "]\n",
    "\n",
    "# Create an empty dictionary to store accuracies\n",
    "accuracies = {}\n",
    "\n",
    "# Iterate through each classifier, train, predict, and store accuracy\n",
    "for name, clf in classifiers:\n",
    "    clf.fit(X_train_imputed, y_train)\n",
    "    predictions = clf.predict(X_test_imputed)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    accuracies[name] = accuracy\n",
    "\n",
    "# Print the accuracies for each model\n",
    "for name, accuracy in accuracies.items():\n",
    "    print(f\"{name} Accuracy: {accuracy}\")\n",
    " \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
